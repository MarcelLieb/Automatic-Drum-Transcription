{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspect training",
   "id": "f718cbf904227e80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import dataset.mapping"
   ],
   "id": "53530f4eb79566ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dataset.datasets import get_dataset\n",
    "from settings import TrainingSettings, AnnotationSettings, AudioProcessingSettings, CNNSettings, EvaluationSettings, \\\n",
    "    DatasetSettings\n",
    "from main import train_epoch, evaluate\n",
    "from model.cnn import CNN"
   ],
   "id": "58116c4c864135f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_settings: TrainingSettings = TrainingSettings(epochs=30, scheduler=False, num_workers=16)\n",
    "audio_settings: AudioProcessingSettings = AudioProcessingSettings()\n",
    "annotation_settings: AnnotationSettings = AnnotationSettings(time_shift=0.0)\n",
    "dataset_settings = DatasetSettings(\n",
    "    audio_settings=audio_settings,\n",
    "    annotation_settings=annotation_settings,\n",
    ")\n",
    "cnn_settings = CNNSettings()\n",
    "evaluation_settings = EvaluationSettings()\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)"
   ],
   "id": "a69a39d184389ef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import color_sequences\n",
    "\n",
    "print(color_sequences)\n",
    "\n",
    "colors = np.array([\n",
    "    [230, 25, 75],\n",
    "    [60, 180, 75],\n",
    "    [255, 225, 25],\n",
    "    [0, 130, 200],\n",
    "    [245, 130, 48],\n",
    "    [145, 30, 180],\n",
    "    [70, 240, 240],\n",
    "    [240, 50, 230],\n",
    "    [210, 245, 60],\n",
    "    [250, 190, 190],\n",
    "    [0, 128, 128],\n",
    "    [230, 190, 255],\n",
    "    [170, 110, 40],\n",
    "    [255, 250, 200],\n",
    "    [128, 0, 0],\n",
    "    [170, 255, 195],\n",
    "    [128, 128, 0],\n",
    "    [255, 215, 180],\n",
    "    [0, 0, 128],\n",
    "    [128, 128, 128],\n",
    "    [0, 0, 0],\n",
    "]) / 255\n",
    "colormap = ListedColormap(colors, N=21)\n",
    "\n",
    "# Plot a pseudo pr curve\n",
    "for i in range(10):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    y = np.sin(x * np.pi * i) * 0.5 + 0.5\n",
    "    plt.plot(x, y, color=colors[i], label=f\"Class {i}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curve\")"
   ],
   "id": "4dbc04e9a5da33f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader_train, loader_val, loader_test_rbma, loader_test_mdb = get_dataset(\n",
    "    training_settings, audio_settings, annotation_settings\n",
    ")"
   ],
   "id": "61db910bb080af78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CNN(**asdict(cnn_settings))\n",
    "model.to(device)"
   ],
   "id": "e161a78cc35b7fba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_lr = training_settings.learning_rate\n",
    "initial_lr = max_lr / 25\n",
    "_min_lr = initial_lr / 1e4\n",
    "initial_lr = (\n",
    "    training_settings.learning_rate\n",
    "    if not training_settings.scheduler\n",
    "    else initial_lr\n",
    ")\n",
    "optimizer = optim.RAdam(\n",
    "    model.parameters(), lr=initial_lr, weight_decay=1e-5\n",
    ")\n",
    "scheduler = (\n",
    "    optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,\n",
    "        steps_per_epoch=len(loader_train),\n",
    "        epochs=training_settings.epochs,\n",
    "    )\n",
    "    if training_settings.scheduler\n",
    "    else None\n",
    ")\n",
    "error = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ],
   "id": "d7262fdde2f89fa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.to(device)\n",
    "best_loss = float(\"inf\")\n",
    "best_score = 0\n",
    "last_improvement = 0\n",
    "for epoch in range(10):\n",
    "    train_loss = train_epoch(\n",
    "        epoch,\n",
    "        loader_train,\n",
    "        device,\n",
    "        None,\n",
    "        error,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        scheduler,\n",
    "        tensorboard_writer=None,\n",
    "    )\n",
    "    val_loss, f_score, avg_f_score = evaluate(\n",
    "        epoch,\n",
    "        model,\n",
    "        loader_val,\n",
    "        error,\n",
    "        device,\n",
    "        evaluation_settings.ignore_beats,\n",
    "        tensorboard_writer=None,\n",
    "        tag=\"Validation\",\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch: {epoch + 1} \"\n",
    "        f\"Loss: {train_loss * 100:.4f}\\t \"\n",
    "        f\"Val Loss: {val_loss * 100:.4f} F-Score: {avg_f_score * 100:.4f}/{f_score * 100:.4f}\"\n",
    "    )"
   ],
   "id": "f42846831e2f2c6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.cpu()\n",
    "loader = iter(loader_test_rbma)"
   ],
   "id": "11af8aa2edcf8f65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spec, target, annotation = next(loader)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(spec)"
   ],
   "id": "b9b727261820ea55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(output.shape)\n",
    "print(target.shape)\n",
    "print(spec.shape)"
   ],
   "id": "e6d2727422c29259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "section = range(1000, 20000)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(output[0][0][section].numpy(), label=\"Output\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5015faf065a93fc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "section = range(1000, 20000)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(output[0][0][section].numpy(), label=\"Output\")\n",
    "plt.plot(target[0][0][section].numpy(), label=\"Target\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "9046e27d3547c1cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from model import ResidualBlock1d  # , CausalConv2d, ResidualBlock\n",
    "\n",
    "eps = np.nextafter(np.float16(0.0), np.float16(1.0))\n",
    "class_idx = 2\n",
    "offset = 1\n",
    "first_label = target[0][class_idx].nonzero()[15]\n",
    "print(first_label)\n",
    "\n",
    "causal_test = spec.clone()\n",
    "causal_test[0, :, :first_label + offset] = 0\n",
    "# causal_test[0, :, first_label+offset+30:] = 0\n",
    "empty_model = ResidualBlock1d(1, 1, kernel_size=5)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # output = model(causal_test)\n",
    "    empty_output = empty_model(causal_test.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "output = empty_output\n",
    "\n",
    "print(torch.min(output[0, class_idx, first_label - offset - 40:first_label + offset]))\n",
    "print(torch.max(output[0, class_idx, first_label - offset - 40:first_label + offset]))\n",
    "print(torch.min(output[0, class_idx, first_label + offset:]))\n",
    "print(torch.max(output[0, class_idx, first_label + offset:]))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(output[0][0][first_label + offset - 10 - 1:first_label + offset + 10].numpy(), label=\"Output\")\n",
    "plt.minorticks_on()"
   ],
   "id": "6d6297ac4bf58933"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test spectogram alignment",
   "id": "65fcacfaa74955d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nnAudio.features\n",
    "from dataset.RBMA13 import RBMA13\n",
    "from dataset import load_audio\n",
    "import torchaudio\n",
    "import librosa\n",
    "from dataset import get_time_index, get_indices"
   ],
   "id": "bdabbf1a09e3ce45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rbma = RBMA13(\n",
    "    path=\"./data/rbma_13\",\n",
    "    settings=dataset_settings,\n",
    "    use_dataloader=False,\n",
    "    is_train=False,\n",
    ")"
   ],
   "id": "a4a30525b6aabfb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mel_log, labels, gt = rbma[2]\n",
    "test_audio = load_audio(rbma.get_full_path(\"RBMA-13-Track-03\"), audio_settings.sample_rate, audio_settings.normalize)\n",
    "gt = gt[2:]"
   ],
   "id": "3851b4bdff3c31a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cqt = nnAudio.features.CQT1992v2(\n",
    "    sr=audio_settings.sample_rate,\n",
    "    hop_length=audio_settings.hop_size,\n",
    "    fmin=20,\n",
    "    fmax=20000,\n",
    "    bins_per_octave=12,\n",
    "    pad_mode=audio_settings.pad_mode,\n",
    ")\n",
    "\n",
    "nnAudio_stft = nnAudio.features.STFT(\n",
    "    n_fft=audio_settings.fft_size,\n",
    "    hop_length=audio_settings.hop_size,\n",
    "    win_length=audio_settings.fft_size // 2,\n",
    "    window=\"hann\",\n",
    "    center=audio_settings.center,\n",
    "    pad_mode=audio_settings.pad_mode,\n",
    "    trainable=False,\n",
    "    output_format=\"Magnitude\",\n",
    ")\n",
    "\n",
    "torch_stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=audio_settings.fft_size,\n",
    "    hop_length=audio_settings.hop_size,\n",
    "    win_length=audio_settings.fft_size // 2,\n",
    "    power=2,\n",
    "    center=audio_settings.center,\n",
    "    pad_mode=audio_settings.pad_mode,\n",
    "    normalized=True,\n",
    "    onesided=True,\n",
    ")\n",
    "\n",
    "mel = torchaudio.transforms.MelScale(\n",
    "    n_mels=audio_settings.n_mels,\n",
    "    sample_rate=audio_settings.sample_rate,\n",
    "    f_min=audio_settings.mel_min,\n",
    "    f_max=audio_settings.mel_max,\n",
    "    n_stft=audio_settings.fft_size // 2 + 1,\n",
    ")"
   ],
   "id": "205fcb8f92853690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cqt_spec = cqt(test_audio)[0]\n",
    "torch_stft_spec = torch_stft(test_audio)\n",
    "nnAudio_stft_spec = nnAudio_stft(test_audio)\n",
    "mel_spec = mel(torch_stft_spec)\n",
    "print(torch_stft_spec.shape)\n",
    "print(nnAudio_stft_spec.shape)\n",
    "print(cqt_spec.shape)\n",
    "print(mel_spec.shape)"
   ],
   "id": "457455519902d117"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(torch_stft_spec.shape[-1] / audio_settings.sample_rate * audio_settings.hop_size)\n",
    "print(nnAudio_stft_spec.shape[-1] / audio_settings.sample_rate * audio_settings.hop_size)\n",
    "print(cqt_spec.shape[-1] / audio_settings.sample_rate * audio_settings.hop_size)\n",
    "print(test_audio.shape[-1] / audio_settings.sample_rate)\n",
    "print(audio_settings.fft_size // audio_settings.hop_size)"
   ],
   "id": "28b86f8a2dea396b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_range = slice(000, 200)\n",
    "\n",
    "gt_labels = [np.array(\n",
    "    [label - plot_range.start / 100 for label in cls if plot_range.start <= int(label * 100) <= plot_range.stop]) for\n",
    "    cls in gt]\n",
    "print(gt_labels)\n",
    "\n",
    "db = librosa.amplitude_to_db(cqt_spec[:, plot_range], ref=np.max)\n",
    "fig, ax = plt.subplots(nrows=3)\n",
    "_img1 = librosa.display.specshow(\n",
    "    db,\n",
    "    y_axis='cqt_hz',\n",
    "    x_axis='time',\n",
    "    ax=ax[0],\n",
    "    sr=audio_settings.sample_rate,\n",
    "    hop_length=audio_settings.hop_size,\n",
    "    fmin=20,\n",
    "    fmax=20000,\n",
    ")\n",
    "ax[0].set(title='CQT spectrogram')\n",
    "\n",
    "M = mel_spec[:, plot_range].detach().numpy()\n",
    "M_db = librosa.power_to_db(M, ref=np.max)\n",
    "_img2 = librosa.display.specshow(\n",
    "    M_db,\n",
    "    y_axis='mel',\n",
    "    x_axis='time',\n",
    "    ax=ax[1],\n",
    "    sr=audio_settings.sample_rate,\n",
    "    hop_length=audio_settings.hop_size,\n",
    "    fmin=audio_settings.mel_min,\n",
    "    fmax=audio_settings.mel_max\n",
    ")\n",
    "ax[1].set(title='Mel spectrogram')\n",
    "\n",
    "time = get_time_index(plot_range.stop - plot_range.start, audio_settings.sample_rate, audio_settings.hop_size)\n",
    "time = np.array([time] * annotation_settings.n_classes).T\n",
    "\n",
    "ax[0].vlines(time[labels[:, plot_range].T == 1], ymin=0, ymax=1)\n",
    "ax[1].vlines(time[labels[:, plot_range].T == 1], ymin=0, ymax=1)\n",
    "ax[2].vlines(time[labels[:, plot_range].T == 1], ymin=0, ymax=1, colors=[\"blue\"])\n",
    "for cls in gt_labels:\n",
    "    ax[2].vlines(cls, ymin=0, ymax=1, colors=[\"red\"])\n",
    "ax[2].set_xlim(left=0, right=time[-1][0])\n",
    "plt.tight_layout()"
   ],
   "id": "e7d4014aed611154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_rate = audio_settings.sample_rate\n",
    "hop_size = audio_settings.hop_size\n",
    "fft_size = audio_settings.fft_size\n",
    "\n",
    "for cls in gt_labels:\n",
    "    if len(cls) == 0:\n",
    "        continue\n",
    "    index = get_indices(cls, sample_rate, hop_size)\n",
    "    time = get_time_index(np.max(index) + 1, sample_rate, hop_size)\n",
    "    assert np.allclose(time[index], cls,\n",
    "                       atol=hop_size / sample_rate / 2), f\"{time[index][np.abs(time[index] - cls) > hop_size / sample_rate / 2]} \\n{cls[np.abs(time[index] - cls) > hop_size / sample_rate / 2]}\"\n",
    "    print()\n"
   ],
   "id": "9b43a325a98c0ca7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test audio loading",
   "id": "57b1a84b4bc8de89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import librosa\n",
    "from dataset import load_audio, get_length\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ],
   "id": "6e0b042456656a10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_audio(audio, sr, title=\"\", offset=0, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        num_samples = len(audio)\n",
    "    t = np.arange(offset, num_samples + offset) / sr\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(t, audio[offset:offset + num_samples])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()"
   ],
   "id": "10989be678c39075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "audio_path = \"./data/a2md_public/ytd_audio/dist0p00/ytd_audio_00040_TRVNNMF128F4285161.mp3\"\n",
    "sample_rate = librosa.get_samplerate(audio_path)\n",
    "duration = get_length(audio_path)\n",
    "print(sample_rate, duration)\n",
    "segment = (12, 20)\n",
    "print(segment[0] * sample_rate, segment[1] * sample_rate)"
   ],
   "id": "3b3cef4e251f476a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import timeit\n",
    "\n",
    "n_runs = 10\n",
    "\n",
    "torch_time = timeit.timeit(\n",
    "    lambda: torchaudio.load(audio_path, frame_offset=int(segment[0] * sample_rate),\n",
    "                            num_frames=int((segment[1] - segment[0]) * sample_rate), backend=\"ffmpeg\", normalize=True),\n",
    "    number=n_runs\n",
    ")\n",
    "\n",
    "librosa_time = timeit.timeit(\n",
    "    lambda: librosa.load(audio_path, sr=sample_rate, offset=segment[0], duration=segment[1] - segment[0]),\n",
    "    number=n_runs\n",
    ")\n",
    "\n",
    "loader_time = timeit.timeit(\n",
    "    lambda: load_audio(path=audio_path, sample_rate=sample_rate, start=segment[0], end=segment[1], normalize=False,\n",
    "                       backend=\"sox\"),\n",
    "    number=n_runs\n",
    ")\n",
    "\n",
    "print(f\"Loader: {loader_time:.4f} s\")\n",
    "print(f\"Torch: {torch_time:.4f} s\")\n",
    "print(f\"Librosa: {librosa_time:.4f} s\")"
   ],
   "id": "b544dd72ec001a98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch_audio = %time torchaudio.load(audio_path, frame_offset=int(segment[0] * sample_rate), num_frames=int((segment[1] - segment[0]) * sample_rate), normalize=True)[0].squeeze()\n",
    "librosa_audio, _ = %time librosa.load(audio_path, sr=sample_rate, offset=segment[0], duration=segment[1] - segment[0])\n",
    "loader_audio = %time load_audio(path=audio_path, sample_rate=sample_rate, start=segment[0], end=segment[1], normalize=False)"
   ],
   "id": "85fcfe1a8882ed9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ref_index = np.argmax(np.abs(librosa_audio))\n",
    "other_ref_index = torch.argmax(torch.abs(torch_audio)).item()\n",
    "if ref_index != other_ref_index:\n",
    "    print(ref_index, other_ref_index)\n",
    "\n",
    "index = torch.argmax(torch.abs(loader_audio)).item()\n",
    "actual_offset = index - ref_index\n",
    "print(actual_offset)\n",
    "print(actual_offset / sample_rate)"
   ],
   "id": "f5ac1d34e55f3d93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(loader_audio.shape, loader_audio.dtype)\n",
    "print(torch_audio.shape, torch_audio.dtype)\n",
    "print(librosa_audio.shape, librosa_audio.dtype)"
   ],
   "id": "ea971d44bf41ac07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(torch.max(loader_audio).item(), torch.min(loader_audio).item())\n",
    "print(torch.max(torch_audio).item(), torch.min(torch_audio).item())\n",
    "print(np.max(librosa_audio), np.min(librosa_audio))\n",
    "print()\n",
    "\n",
    "difference = loader_audio.shape[-1] - torch_audio.shape[-1]\n",
    "if difference != 0:\n",
    "    print(f\"Loader audio is longer by {difference} samples\")\n",
    "    # loader_audio = loader_audio[difference:]\n",
    "\n",
    "if not np.allclose(librosa_audio, torch_audio.numpy(), atol=1e-5):\n",
    "    print(f\"Librosa and torch audio are not equal\")\n",
    "    print(np.max(np.abs(librosa_audio - torch_audio.numpy())))\n",
    "if not np.allclose(loader_audio.numpy(), torch_audio.numpy(), atol=1e-5):\n",
    "    print(f\"Loader and torch audio are not equal\")\n",
    "    print(np.max(np.abs(loader_audio.numpy() - torch_audio.numpy())))\n",
    "if not np.allclose(loader_audio, librosa_audio, atol=1e-5):\n",
    "    print(f\"Loader and librosa audio are not equal\")\n",
    "    print(np.max(np.abs(loader_audio.numpy() - librosa_audio)))"
   ],
   "id": "bbd739637629e5e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "abs_error = 1e-4\n",
    "mask = (~(np.isclose(loader_audio.numpy(), librosa_audio, atol=abs_error))).nonzero()\n",
    "print(mask, len(mask[0]))\n",
    "errors = loader_audio[~np.isclose(loader_audio.numpy(), librosa_audio, atol=abs_error)]\n",
    "print(errors.shape, errors.shape[-1] / loader_audio.shape[-1])"
   ],
   "id": "4874d57710df63e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if difference > 0:\n",
    "    extra_audio = loader_audio[:difference]\n",
    "    extra_audio = extra_audio / torch.max(extra_audio)\n",
    "    ipd.Audio(extra_audio, rate=sample_rate)"
   ],
   "id": "c666e1428c7d58f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_samples = 800\n",
    "offset = 0"
   ],
   "id": "52a1462f889d6af8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_audio(loader_audio, sample_rate, \"Loader audio\", num_samples=num_samples, offset=offset)\n",
    "plot_audio(torch_audio.numpy(), sample_rate, \"Torch audio\", num_samples=num_samples, offset=offset)\n",
    "plot_audio(librosa_audio, sample_rate, \"Librosa audio\", num_samples=num_samples, offset=offset)"
   ],
   "id": "46a81517f57f1636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ipd.Audio(loader_audio.numpy(), rate=sample_rate)",
   "id": "31b9d5b6bb6b5578"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset inspection",
   "id": "62337c466f14ec29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pretty_midi import PrettyMIDI\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "from dataset.A2MD import get_tracks"
   ],
   "id": "4a650916eb340ef8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "midi = PrettyMIDI(\"./data/a2md_public/align_mid/dist0p00/align_mid_00012_TRYIYUF128F932573D.mid\")",
   "id": "d31a659af740b382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_drum_pitch_velocity(_midi: PrettyMIDI) -> np.ndarray:\n",
    "    drum_instruments: list[pretty_midi.Instrument] = [\n",
    "        instrument for instrument in _midi.instruments if instrument.is_drum\n",
    "    ]\n",
    "    notes = np.array(\n",
    "        [\n",
    "            (note.pitch, note.velocity)\n",
    "            for instrument in drum_instruments\n",
    "            for note in instrument.notes\n",
    "        ]\n",
    "    )\n",
    "    return notes\n",
    "\n",
    "\n",
    "np.array(drum_instruments[0])\n",
    "# np.unique(notes[:, 0].astype(int), return_counts=True)"
   ],
   "id": "f061d108c2048665"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inspect trained model",
   "id": "b58b31141180d274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.RBMA13 import RBMA13\n",
    "from dataset.A2MD import A2MD\n",
    "from settings import DatasetSettings, CNNMambaSettings, TrainingSettings, CNNSettings, CNNAttentionSettings\n",
    "from dataclasses import asdict\n",
    "import dataset\n",
    "from evallib import peak_pick_max_mean, calculate_pr\n",
    "\n",
    "from model.CRNN import CRNN\n",
    "from model.unet import UNet\n",
    "from model.cnnM2 import CNNMambaFast\n",
    "from model.cnnM import CNNMamba\n",
    "from model.cnnA import CNNAttention"
   ],
   "id": "f28a989b0fecf9e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "checkpoint = torch.load(\"../models/Jun04_18-08-56_marclie-desktop_mamba_fast_66.71.pt\", map_location=torch.device(\"cpu\"))",
   "id": "42d2638a9f5f03bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.cuda.empty_cache()\n",
    "dataset_settings = DatasetSettings.from_flat_dict(checkpoint[\"dataset_settings\"])\n",
    "dataset_settings.segment_type = None\n",
    "training_settings = TrainingSettings.from_flat_dict(checkpoint[\"training_settings\"])\n",
    "model_settings = training_settings.get_model_settings_class().from_flat_dict(checkpoint[\"model_settings\"])\n",
    "match training_settings.model_settings:\n",
    "    case \"cnn\":\n",
    "        model = CNN(**asdict(model_settings), n_classes=dataset_settings.annotation_settings.n_classes,\n",
    "                    n_mels=checkpoint[\"dataset_settings\"][\"n_mels\"])\n",
    "    case \"cnn_attention\":\n",
    "        model = CNNAttention(**asdict(model_settings), n_classes=dataset_settings.annotation_settings.n_classes,\n",
    "                             n_mels=checkpoint[\"dataset_settings\"][\"n_mels\"])\n",
    "    case \"mamba\":\n",
    "        model = CNNMamba(**asdict(model_settings), n_classes=dataset_settings.annotation_settings.n_classes,\n",
    "                         n_mels=checkpoint[\"dataset_settings\"][\"n_mels\"])\n",
    "    case \"mamba_fast\":\n",
    "        model = CNNMambaFast(**asdict(model_settings), n_classes=dataset_settings.annotation_settings.n_classes,\n",
    "                             n_mels=checkpoint[\"dataset_settings\"][\"n_mels\"])\n",
    "    case \"unet\":\n",
    "        model = UNet(**asdict(model_settings))\n",
    "    case \"crnn\":\n",
    "        model = CRNN(**asdict(model_settings), n_classes=dataset_settings.annotation_settings.n_classes,\n",
    "                     n_mels=checkpoint[\"dataset_settings\"][\"n_mels\"])\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid model setting\")\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model = model.eval()"
   ],
   "id": "e2058bb5ec43e9c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "training_settings",
   "id": "505c1f9e3a42525e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rbma = RBMA13(\n",
    "    path=\"../data/rbma_13\",\n",
    "    settings=dataset_settings,\n",
    "    use_dataloader=True,\n",
    "    is_train=False,\n",
    "    segment=False,\n",
    ")\n",
    "\n",
    "rbma_loader = dataset.get_dataloader(rbma, 1, 1, is_train=False)\n",
    "\n",
    "# a2md = A2MD(\n",
    "#     path=\"data/a2md_public\",\n",
    "#     settings=dataset_settings,\n",
    "#     use_dataloader=False,\n",
    "#     is_train=False,\n",
    "#     segment=False,\n",
    "# )"
   ],
   "id": "5fa5396d266adeaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mel_log, labels, gt = next(iter(rbma_loader))\n",
    "print(mel_log.shape, labels.shape, len(gt[0]))\n",
    "gt = gt[0][2:]"
   ],
   "id": "60ce5b160cbb131e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "next(iter(rbma_loader))",
   "id": "92ded97262d785fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = model.cuda()\n",
    "output: torch.Tensor = model(mel_log.cuda()).detach().cpu().sigmoid().squeeze(0)\n",
    "print(output.shape)"
   ],
   "id": "5566d8f8da79d58b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "peaks = peak_pick_max_mean(output.unsqueeze(0), dataset_settings.audio_settings.sample_rate, dataset_settings.audio_settings.hop_size, dataset_settings.audio_settings.fft_size)\n",
    "for cls in peaks:\n",
    "    for peak in cls:\n",
    "        peak[0] -= dataset_settings.annotation_settings.time_shift\n",
    "_, _, _, score_sum, score_avg, _ = calculate_pr(peaks, [gt], detection_window=0.025)\n",
    "score_avg, score_sum"
   ],
   "id": "668a90331ffc6544"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = model.cuda()\n",
    "with torch.inference_mode():\n",
    "    segment = mel_log[..., 6200:7000]\n",
    "    print(segment.min(), segment.max())\n",
    "    segment = segment.cuda()\n",
    "    # for some reason, the output is dependent on the segment length\n",
    "    expected_output = model(segment)[..., :400].detach().cpu().squeeze(0)\n",
    "    # plt.plot(expected_output[0].numpy(), scaley=False)\n",
    "    for i in range(100):\n",
    "        # segment = (torch.rand_like(segment) * 8 + 1).log()\n",
    "        random = (torch.rand_like(segment) * 8 + 1).log()\n",
    "        in_seg = torch.concatenate([segment, random], dim=-1)\n",
    "        out_mixed = model(in_seg)[..., :400].detach().cpu().squeeze(0)\n",
    "        # plt.plot((out_mixed - expected_output).abs()[0].numpy(), scaley=True)\n",
    "        # assert torch.allclose(expected_output, out_clean, atol=1e-7), f\"{(out_clean - expected_output).abs().sum().item(), (out_clean - expected_output).argmax(dim=-1)}\"\n",
    "        assert torch.allclose(out_mixed, expected_output,\n",
    "                              atol=1e-9), f\"{(out_mixed - expected_output).abs().sum().item(), (out_mixed - expected_output).argmax(dim=-1)}\""
   ],
   "id": "4b34a74965a95da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gt_labels = dataset.get_labels(gt, dataset_settings.audio_settings.sample_rate, dataset_settings.audio_settings.hop_size, labels.shape[-1])\n",
    "plt.plot(output[0, 7230:7230 + 10].numpy(), scaley=False)\n",
    "plt.plot(labels[0, 0, 7230:7230 + 10].numpy(), scaley=False)\n",
    "plt.plot(gt_labels[0, 7230:7230 + 10].numpy(), scaley=False)"
   ],
   "id": "8e9d1a91a1f7c955"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_pos, num_neg = a2md.get_sample_distribution()\n",
    "# weights according to https://markcartwright.com/files/cartwright2018increasing.pdf section 3.4.1 Task weights\n",
    "total = (num_pos + num_neg)[0]\n",
    "print(num_pos)\n",
    "p_i = num_pos / (total * dataset_settings.annotation_settings.n_classes)\n",
    "weight = 1 / (-p_i * p_i.log() - (1 - p_i) * (1 - p_i).log())\n",
    "weight"
   ],
   "id": "3b41ea3cd5efaf7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Causal Blocks",
   "id": "5cffcfe789b0ba1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from model import CausalAvgPool1d, CausalConv1d, CausalConv2d, CausalMaxPool1d, ResidualBlock\n",
    "from model.cnn_feature import CNNFeature\n",
    "from functools import partial"
   ],
   "id": "cf98ae631f1cc1b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "blocks_to_test = [\n",
    "    partial(torch.nn.Conv2d, 100, 32, 1),\n",
    "    partial(torch.nn.MaxPool2d, kernel_size=(2, 1), stride=(2, 1)),\n",
    "    partial(torch.nn.Dropout2d, 0.3),\n",
    "    partial(ResidualBlock, 100, 10, 3),\n",
    "    partial(CNNFeature, num_channels=16, n_layers=2, down_sample_factor=2, channel_multiplication=2, activation=torch.nn.ReLU(), causal=True, dropout=0.3, in_channels=100)\n",
    "]"
   ],
   "id": "2bcff8c7af5c5840"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.use_deterministic_algorithms(True)\n",
    "compare_size = 64\n",
    "for block in blocks_to_test:\n",
    "    mod = block().eval()\n",
    "    print(f\"Testing: {mod.__class__}\")\n",
    "    for name, param in mod.named_parameters():\n",
    "        # Set weights to one\n",
    "        if 'weight' in name:\n",
    "              param.data = torch.ones_like(param)\n",
    "        elif 'bias' in name:\n",
    "              param.data = torch.ones_like(param)\n",
    "    with torch.inference_mode():\n",
    "        for i in range(100):\n",
    "            a = torch.rand(1, 100, 10, compare_size)\n",
    "            b = torch.concatenate([a, torch.rand_like(a)[..., :]], dim=-1)\n",
    "            assert (mod(a)[..., :compare_size] == mod(b)[..., :compare_size]).all(), f\"Error in iteration {i}: {(mod(a)[..., :compare_size] - mod(b)[..., :compare_size]).abs().sum()}\""
   ],
   "id": "7745f3457d8e1a3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Learning rate finder",
   "id": "9d869628fc9d94c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "from dataset.datasets import get_dataset\n",
    "from hyperparameters import final_experiment_params\n",
    "from settings import Config\n",
    "from model.CRNN import CRNN\n",
    "from model.unet import UNet\n",
    "from model.cnnM2 import CNNMambaFast\n",
    "from model.cnnM import CNNMamba\n",
    "from model.cnnA import CNNAttention\n",
    "\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter, ValDataLoaderIter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "id": "4fbb25ce774aa377"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "# get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "current_working_directory"
   ],
   "id": "29c1a8904df3b427"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomTrainIter(TrainDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        return batch_data[0], batch_data[1]\n",
    "\n",
    "class CustomValIter(ValDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        return batch_data[0], batch_data[1]"
   ],
   "id": "8cc4710fbdd25f4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = Config.from_flat_dict(final_experiment_params[\"Mamba fast\"])\n",
    "\n",
    "model_settings = config.model\n",
    "match config.training.model_settings:\n",
    "    case \"cnn\":\n",
    "        model = CNN(**asdict(model_settings), n_classes=config.dataset.annotation_settings.n_classes,\n",
    "                    n_mels=config.dataset.audio_settings.n_mels)\n",
    "    case \"cnn_attention\":\n",
    "        model = CNNAttention(**asdict(model_settings), n_classes=config.dataset.annotation_settings.n_classes,\n",
    "                             n_mels=config.dataset.audio_settings.n_mels)\n",
    "    case \"mamba\":\n",
    "        model = CNNMamba(**asdict(model_settings), n_classes=config.dataset.annotation_settings.n_classes,\n",
    "                         n_mels=config.dataset.audio_settings.n_mels)\n",
    "    case \"mamba_fast\":\n",
    "        model = CNNMambaFast(**asdict(model_settings), n_classes=config.dataset.annotation_settings.n_classes,\n",
    "                             n_mels=config.dataset.audio_settings.n_mels)\n",
    "    case \"unet\":\n",
    "        model = UNet(**asdict(model_settings))\n",
    "    case \"crnn\":\n",
    "        model = CRNN(**asdict(model_settings), n_classes=config.dataset.annotation_settings.n_classes,\n",
    "                     n_mels=config.dataset.audio_settings.n_mels)\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid model setting\")"
   ],
   "id": "53755f1a384eab01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config.training.dataset_version = \"S\"\n",
    "config.training.test_sets = ()\n",
    "\n",
    "train_loader, val_loader, _ = get_dataset(config.training, config.dataset)"
   ],
   "id": "6f5590bd059f3332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.RAdam(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    betas=(config.training.beta_1, config.training.beta_2),\n",
    "    weight_decay=config.training.weight_decay,\n",
    "    decoupled_weight_decay=config.training.decoupled_weight_decay,\n",
    "    eps=config.training.epsilon,\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "amp_config = {\n",
    "    'device_type': 'cuda',\n",
    "    'dtype': torch.float16,\n",
    "}\n",
    "grad_scaler = torch.amp.GradScaler(\"cuda\")\n"
   ],
   "id": "f42f39a4ce629e2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr_finder = LRFinder(\n",
    "    model, optimizer, criterion, device='cuda',\n",
    "    amp_backend='torch', amp_config=amp_config, grad_scaler=grad_scaler\n",
    ")\n",
    "lr_finder.range_test(train_loader, val_loader=val_loader, start_lr=1e-6, end_lr=0.1, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ],
   "id": "85bd6289c29dbf9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(range(10))\n",
    "plt.show()"
   ],
   "id": "42a61320bc86a6d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
